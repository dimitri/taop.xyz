<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Archiving on The Art of PostgreSQL</title>
    <link>https://theartofpostgresql.com/tags/archiving/</link>
    <description>Recent content in Archiving on The Art of PostgreSQL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2019 10:50:00 +0200</lastBuildDate>
    
	<atom:link href="https://theartofpostgresql.com/tags/archiving/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Compute database size</title>
      <link>https://theartofpostgresql.com/compute-database-size/</link>
      <pubDate>Mon, 02 Sep 2019 10:50:00 +0200</pubDate>
      
      <guid>https://theartofpostgresql.com/compute-database-size/</guid>
      <description>Photo by unsplash-logoCharles ðŸ‡µðŸ‡­  It is well known that database design should be as simple as possible, and follow the normalization process. Except in some cases, sometimes, for scalability purposes. Partitioning might be used to help deal with large amount of data for instance.
But what is a large amount of data? Do you need to pay attention to those scalability trade-offs now, or can you wait until later?</description>
    </item>
    
  </channel>
</rss>